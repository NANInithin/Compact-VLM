# üçå Mitigating Hallucination in Compact VLMs via Chain-of-Thought

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/release/python-3100/)
[![Model: SmolVLM2](https://img.shields.io/badge/Model-SmolVLM2_2.2B-red)](https://huggingface.co/HuggingFaceTB/SmolVLM2-2.2B-Instruct)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> **Semester Project:** Investigating visual reliability in compact Vision-Language Models (VLMs) and proposing a "Visual Audit" prompting strategy to eliminate hallucination.

---

## üßê The Problem

Compact VLMs (like SmolVLM2, <3B params) are efficient for edge deployment but suffer from **"Language Priors."** They often ignore visual evidence in favor of what they *expect* to see (e.g., assuming a banana must have a sticker or be in a bowl).

This project investigates:

1. **Blindness vs. Hallucination:** Is the model blind, or is it just suggestible?
2. **The "Purple Banana" Test:** Can it see counter-factual colors? (Result: ‚úÖ Yes)
3. **The "Phantom Object" Trap:** Can we trick it into inventing objects? (Result: ‚ùå Yes)
4. **The Fix:** Using **Chain-of-Thought (CoT)** to force a visual audit.

## üìä Key Results

| Experiment Type | Sticker (Hallucination) | Apple (Co-occurrence) | Bowl (Visual Ambiguity) |
| :--- | :--- | :--- | :--- |
| **Standard Inference** | ‚ùå Fails (Invents text) | ‚ùå Fails (Invents apple) | ‚ùå Fails (Says "Ceramic") |
| **Defensive Prompt** | ‚úÖ Fixed | ‚úÖ Fixed | ‚ùå Fails (Still says "Ceramic") |
| **CoT Visual Audit** | ‚úÖ **Fixed** | ‚úÖ **Fixed** | ‚úÖ **Fixed** |

---

## üõ†Ô∏è Installation

### 1. Clone the repository

```bash
git clone https://github.com/yourusername/vlm-hallucination-project.git
cd vlm-hallucination-project
```

### 2. Set up the Environment

```bash
# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies (Optimized for CUDA 12.8)
pip install -r requirements.txt
```

### 3. Generate Test Data

We use a script to generate the adversarial "Purple Banana" and verify the "Real Banana" exists.

```bash
python src/generate_trap_real.py
```

## üß™ Experiments

### 1Ô∏è‚É£ The "Purple Banana" Test (Modality Check)

Tests if the model actually looks at the image or just guesses colors based on the object name.

```bash
python src/experiment.py
```

**Hypothesis:** If it says "Yellow" for a purple banana, it has Modality Collapse.

**Result:** It correctly identified "Purple/Pink," proving the Vision Encoder is robust.

### 2Ô∏è‚É£ The "Phantom Object" Test (Baseline)

Tests suggestibility by asking about non-existent items (stickers, bowls).

```bash
python src/experiment_phantom.py
```

**Result:** The model hallucinated a sticker text and a ceramic bowl.

### 3Ô∏è‚É£ The "Chain-of-Thought" Fix (Solution)

Implements a 2-step "Visual Audit" prompt:

1. List visible objects.
2. Answer based ONLY on that list.

```bash
python src/experiment_cot.py
```

**Result:** Hallucinations dropped to 0%.

## üìÇ Repository Structure

```
‚îú‚îÄ‚îÄ data/                  # Experiment images (generated locally)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ experiment.py      # Basic adversarial color test
‚îÇ   ‚îú‚îÄ‚îÄ experiment_phantom.py # Hallucination probing
‚îÇ   ‚îú‚îÄ‚îÄ experiment_fix.py  # Attempt 1: Defensive Prompting
‚îÇ   ‚îú‚îÄ‚îÄ experiment_cot.py  # Attempt 2: Chain of Thought (The Solution)
‚îÇ   ‚îî‚îÄ‚îÄ generate_trap_real.py # Data generation script
‚îú‚îÄ‚îÄ requirements.txt       # Dependencies
‚îî‚îÄ‚îÄ README.md              # Project documentation
```

## ü§ù Acknowledgments

- **Model:** SmolVLM2-2.2B-Instruct by Hugging Face TB.
- **Hardware:** Experiments ran on NVIDIA RTX 4060 Laptop GPU (8GB VRAM).
